import csv
from selenium import webdriver
from selenium.webdriver.support.ui import Select
from bs4 import BeautifulSoup
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC


url = "https://www.famcap.com/top-500-german-family-businesses-the-economy-most-dependent-on-family-enterprises"

# Start a headless Chrome browser
options = webdriver.ChromeOptions()
options.add_argument('headless')
driver = webdriver.Chrome(options=options)

# Load the webpage
driver.get(url)

# Find the drop-down menu and select the maximum number of rows
#select = Select(driver.find_element(By.XPATH, "/html/body/div[3]/div/div[2]/div[1]/div/div/div/div/div/div[3]/label/div/button"))
#select.select_by_value('All')  # replace 'MAX' with the value for the maximum number of rows

WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, "/html/body/div[3]/div/div[2]/div[1]/div/div/div/div/div/div[3]/label/div"))).click()
#WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, "//*[@id="table_1_length"]/label/div/div"))).click()
WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, "/html/body/div[3]/div/div[2]/div[1]/div/div/div/div/div/div[3]/label/div/div/ul/li[7]"))).click()


# Wait for the page to update with the new number of rows
# (you might need to adjust the sleep time depending on the page)
#time.sleep(5)

# Scrape the entire table using BeautifulSoup
soup = BeautifulSoup(driver.page_source, 'html.parser')
table = soup.find('table')

data = []
rows = table.find_all('tr')
for row in rows:
    cols = row.find_all('td')
    cols = [col.text.strip() for col in cols]
    data.append(cols)

# Write the data to a CSV file
with open('output.csv', 'w', newline='') as file:
    writer = csv.writer(file)
    writer.writerows(data)

# Quit the browser
driver.quit()

